"""
Logging extension for capturing spider errors and logs and saving them to the database.
"""
from __future__ import annotations

import logging
import threading
from typing import List

from scrapy import signals
from scrapy.spiders import Spider
from scrapy.crawler import Crawler

from .base import BaseExtension

logger = logging.getLogger(__name__)


class DatabaseLogHandler(logging.Handler):
    """
    Custom logging handler to save log records to the database in batches.
    """
    _local = threading.local()

    def __init__(self, extension: 'LoggingExtension', spider: Spider, batch_size: int):
        super().__init__()
        self.extension = extension
        self.spider = spider
        self.batch_size = batch_size
        self._buffer: List[tuple[Spider, str, str]] = []

    def emit(self, record: logging.LogRecord):
        if getattr(self._local, 'in_emit', False):
            return  # Prevent recursion

        # Avoid capturing logs generated by this extension's own exceptions
        if 'extensions/logging.py' in record.pathname:
            return

        self._local.in_emit = True
        try:
            msg = self.format(record)
            level = record.levelname
            self._buffer.append((self.spider, level, msg))
            if len(self._buffer) >= self.batch_size:
                self.flush()
        except Exception:
            # Use logger directly to avoid recursion if formatting fails
            logger.exception("Error in DatabaseLogHandler.emit")
        finally:
            self._local.in_emit = False

    def flush(self):
        if not self._buffer:
            return
        
        records_to_send = self._buffer
        self._buffer = []

        try:
            for spider, level, msg in records_to_send:
                self.extension._log_to_database(spider, level, msg)
        except Exception:
            logger.exception("Failed to flush log buffer to database.")

    def close(self):
        """
        Flush any buffered logs and close the handler.
        """
        self.flush()
        super().close()


class LoggingExtension(BaseExtension):
    """
    Extension for logging spider events to the database.
    """

    def __init__(self, settings):
        super().__init__(settings)
        crawler_settings = self.settings.crawler_settings
        self.log_level = crawler_settings.get('LOG_LEVEL', 'INFO').upper()
        # Use hardcoded defaults for batch size and format
        self.batch_size = 5
        self.log_format = '%(asctime)s [%(name)s] %(levelname)s: %(message)s'
        self.log_dateformat = '%Y-%m-%d %H:%M:%S'

        self._db_log_handler: DatabaseLogHandler | None = None
        self._root_logger_ref: logging.Logger | None = None

    @classmethod
    def from_crawler(cls, crawler: Crawler):
        """Create an extension instance from crawler."""
        ext = super().from_crawler(crawler)
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.spider_error, signal=signals.spider_error)
        crawler.signals.connect(ext.item_dropped, signal=signals.item_dropped)
        crawler.signals.connect(ext.engine_stopped, signal=signals.engine_stopped)
        return ext

    def spider_opened(self, spider: Spider):
        """Called when a spider is opened."""
        handler = DatabaseLogHandler(self, spider, self.batch_size)
        level = getattr(logging, self.log_level, logging.INFO)
        handler.setLevel(level)
        formatter = logging.Formatter(fmt=self.log_format, datefmt=self.log_dateformat)
        handler.setFormatter(formatter)
        self._db_log_handler = handler

        # Attach the handler ONLY to the root logger.
        # This captures all logs from all libraries due to propagation.
        root_logger = logging.getLogger()
        
        # Avoid adding duplicate handlers if a spider is run multiple times in the same process
        if not any(isinstance(h, DatabaseLogHandler) for h in root_logger.handlers):
            root_logger.addHandler(handler)
            self._root_logger_ref = root_logger

        identifier_column, identifier_value = self.get_identifier_info(spider)
        message = f"{identifier_column.title()} {identifier_value} started"
        spider.logger.info(message)

    def spider_closed(self, spider: Spider, reason: str):
        """Called when a spider is closed."""
        identifier_column, identifier_value = self.get_identifier_info(spider)
        message = f"{identifier_column.title()} {identifier_value} closed with reason: {reason}"
        spider.logger.info(message)
        self._cleanup()

    def engine_stopped(self):
        """Called when the Scrapy engine stops to ensure logs are flushed."""
        self._cleanup()

    def _cleanup(self):
        """Removes the log handler and flushes any remaining logs."""
        if self._db_log_handler and self._root_logger_ref:
            self._root_logger_ref.removeHandler(self._db_log_handler)
            self._db_log_handler.close()

            self._db_log_handler = None
            self._root_logger_ref = None

    def spider_error(self, failure, response, spider: Spider):
        """Logs spider errors to the database."""
        message = f"Spider error on {response.url}:\n{failure.getTraceback()}"
        spider.logger.error(message)

    def item_dropped(self, item, response, spider: Spider, exception: Exception):
        """Logs dropped items to the database."""
        message = f"Item dropped: {exception} from {response.url}"
        spider.logger.warning(message)
